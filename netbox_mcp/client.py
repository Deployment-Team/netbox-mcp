#!/usr/bin/env python3
"""
NetBox Dynamic API Client for NetBox MCP Server

Revolutionary dynamic client architecture providing 100% NetBox API coverage through 
intelligent proxy pattern. Eliminates the need for manual method implementations by 
dynamically routing all requests to the appropriate NetBox API endpoints.

**Architecture Components:**
- NetBoxClient: Dynamic entrypoint with __getattr__ routing
- AppWrapper: Navigator between NetBox apps (dcim, ipam, circuits)  
- EndpointWrapper: Executor with enterprise-grade caching and safety

**Key Features:**
- 100% NetBox API coverage automatically
- Enterprise-grade safety mechanisms (confirm=True, dry-run mode)
- TTL-based caching with obj.serialize() data integrity
- Future-proof against NetBox API changes
- Thread-safe operations with comprehensive audit logging

**Usage Examples:**
    client = NetBoxClient(config)
    
    # Dynamic API access - every endpoint available
    manufacturers = client.dcim.manufacturers.all()
    sites = client.dcim.sites.filter(status="active")
    devices = client.dcim.devices.filter(site="datacenter-1")
    
    # Write operations with safety
    client.dcim.manufacturers.create(name="Cisco", confirm=True)
    client.dcim.devices.update(device_id, status="offline", confirm=True)
"""

import logging
import time
from typing import Dict, List, Optional, Any
from dataclasses import dataclass

import pynetbox
import requests
from cachetools import TTLCache

from .config import NetBoxConfig
from .exceptions import (
    NetBoxError,
    NetBoxConnectionError,
    NetBoxAuthError,
    NetBoxValidationError,
    NetBoxNotFoundError,
    NetBoxPermissionError,
    NetBoxWriteError,
    NetBoxConfirmationError
)

logger = logging.getLogger(__name__)


@dataclass
class ConnectionStatus:
    """NetBox connection status information."""
    connected: bool
    version: Optional[str] = None
    python_version: Optional[str] = None
    django_version: Optional[str] = None
    plugins: Optional[Dict[str, str]] = None
    response_time_ms: Optional[float] = None
    cache_stats: Optional[Dict[str, Any]] = None
    error: Optional[str] = None


class CacheManager:
    """
    Cache manager implementing Gemini's caching strategy.
    
    Provides TTL-based caching with configurable TTLs per object type,
    standardized cache key generation, and comprehensive metrics tracking.
    """
    
    def __init__(self, config: NetBoxConfig):
        """Initialize cache with configuration."""
        self.config = config
        self.enabled = config.cache.enabled
        
        # --- GEMINI'S FIX ---
        # Always initialize self.caches as empty dictionary to prevent AttributeError
        self.caches = {}
        self.default_cache = None
        self.stats = {
            "hits": 0,
            "misses": 0,
            "evictions": 0,
            "invalidations": 0
        }
        
        # Add thread safety lock
        import threading
        self.lock = threading.Lock()
        
        if self.enabled:
            logger.info("Cache is enabled. Initializing per-type TTL caches.")
            
            # Create TTL caches for each object type with specific TTLs
            object_types = [
                ("dcim.manufacturer", config.cache.ttl.manufacturers),
                ("dcim.site", config.cache.ttl.sites),
                ("dcim.device_role", config.cache.ttl.device_roles),
                ("dcim.device_type", config.cache.ttl.device_types),
                ("dcim.device", config.cache.ttl.devices)
            ]
            
            for obj_type, ttl in object_types:
                self.caches[obj_type] = TTLCache(maxsize=config.cache.max_items // len(object_types), ttl=ttl)
            
            # Default cache for other object types
            self.default_cache = TTLCache(maxsize=config.cache.max_items // 4, ttl=config.cache.ttl.default)
            
            logger.info(f"Cache initialized: enabled={self.enabled}, max_items={config.cache.max_items}, caches={len(self.caches)}")
        else:
            logger.info("Cache disabled by configuration")
    
    def generate_cache_key(self, object_type: str, **kwargs) -> str:
        """
        Generate standardized cache key following Gemini's schema.
        
        Format: "<object_type>:<param1>=<value1>:<param2>=<value2>"
        Parameters are sorted alphabetically for consistency.
        
        Args:
            object_type: NetBox object type (e.g., "dcim.device", "dcim.manufacturer")
            **kwargs: Filter parameters for the query
            
        Returns:
            Standardized cache key string
        """
        if not kwargs:
            return object_type
        
        # Sort parameters for consistent key generation
        sorted_params = sorted(kwargs.items())
        param_str = ":".join(f"{k}={v}" for k, v in sorted_params if v is not None)
        
        return f"{object_type}:{param_str}" if param_str else object_type
    
    def get_ttl_for_object_type(self, object_type: str) -> int:
        """Get TTL for specific object type from configuration."""
        type_mapping = {
            "dcim.manufacturer": self.config.cache.ttl.manufacturers,
            "dcim.site": self.config.cache.ttl.sites,
            "dcim.devicerole": self.config.cache.ttl.device_roles,
            "dcim.devicetype": self.config.cache.ttl.device_types,
            "dcim.device": self.config.cache.ttl.devices,
            "ipam.ipaddress": self.config.cache.ttl.ip_addresses,
            "dcim.interface": self.config.cache.ttl.device_interfaces,
            "ipam.vlan": self.config.cache.ttl.vlans,
            "status": self.config.cache.ttl.status,
            "health": self.config.cache.ttl.health
        }
        
        return type_mapping.get(object_type, self.config.cache.ttl.default)
    
    def get(self, cache_key: str, object_type: str) -> Optional[Any]:
        """Get item from cache with metrics tracking."""
        if not self.enabled:
            return None
        
        try:
            # Thread-safe cache access
            with self.lock:
                # Get appropriate cache for object type
                cache = self.caches.get(object_type, self.default_cache)
                if cache is None:
                    self.stats["misses"] += 1
                    return None
                
                # Check if item exists and is not expired
                if cache_key in cache:
                    self.stats["hits"] += 1
                    logger.debug(f"Cache HIT: {cache_key}")
                    return cache[cache_key]
                else:
                    self.stats["misses"] += 1
                    logger.debug(f"Cache MISS: {cache_key}")
                    return None
                
        except Exception as e:
            logger.warning(f"Cache get error for key {cache_key}: {e}")
            return None
    
    def set(self, cache_key: str, value: Any, object_type: str) -> None:
        """Set item in cache with object-specific TTL."""
        if not self.enabled:
            logger.debug(f"Cache SET SKIPPED: cache disabled")
            return
        
        try:
            # Thread-safe cache access
            with self.lock:
                # Get appropriate cache for object type
                cache = self.caches.get(object_type) if self.caches else None
                
                if cache is None:
                    # Try default cache as fallback
                    cache = self.default_cache
                
                if cache is None:
                    logger.warning(f"Cache SET FAILED: no cache found for object_type {object_type}")
                    logger.debug(f"Available cache types: {list(self.caches.keys()) if self.caches else 'No caches dict'}")
                    return
                
                logger.debug(f"Cache SET ATTEMPT: {cache_key} in {object_type} cache (size before: {len(cache)})")
                
                # Store in appropriate TTL cache
                cache[cache_key] = value
                
                logger.debug(f"Cache SET SUCCESS: {cache_key} in {object_type} cache (size after: {len(cache)})")
                
                # Get TTL for logging
                ttl = self.get_ttl_for_object_type(object_type)
                logger.info(f"Cache SET: {cache_key} (TTL: {ttl}s)")
            
        except Exception as e:
            logger.error(f"Cache set error for key {cache_key}: {e}", exc_info=True)
    
    def invalidate_pattern(self, pattern: str) -> int:
        """
        Invalidate cache entries matching pattern.
        
        Args:
            pattern: Pattern to match (e.g., "dcim.device" to invalidate all devices)
            
        Returns:
            Number of entries invalidated
        """
        if not self.enabled:
            return 0
        
        try:
            total_invalidated = 0
            
            # Thread-safe cache access
            with self.lock:
                # Check all cache instances
                all_caches = list(self.caches.values())
                if self.default_cache:
                    all_caches.append(self.default_cache)
                
                for cache in all_caches:
                    keys_to_remove = [key for key in cache.keys() if pattern in key]
                    
                    for key in keys_to_remove:
                        del cache[key]
                        self.stats["invalidations"] += 1
                        total_invalidated += 1
            
            logger.debug(f"Cache invalidated {total_invalidated} entries matching pattern: {pattern}")
            return total_invalidated
            
        except Exception as e:
            logger.warning(f"Cache invalidation error for pattern {pattern}: {e}")
            return 0
    
    def invalidate_for_object(self, object_type: str, object_id: int) -> int:
        """
        Invalidate all cache entries containing a specific object ID.
        
        This method invalidates all cached queries that might contain the specified object,
        ensuring data consistency after write operations that affect the object.
        
        Args:
            object_type: NetBox object type (e.g., "dcim.interface", "dcim.device")
            object_id: ID of the object that was modified
            
        Returns:
            Number of cache entries invalidated
        """
        if not self.enabled:
            return 0
        
        try:
            total_invalidated = 0
            
            # Thread-safe cache access
            with self.lock:
                # Check all cache instances
                all_caches = list(self.caches.values())
                if self.default_cache:
                    all_caches.append(self.default_cache)
                
                # Invalidate all cache entries that might contain this object
                # This includes both direct lookups and filtered queries
                patterns_to_check = [
                    f"{object_type}:id={object_id}",  # Direct ID lookup
                    f"{object_type}:",                # Any query for this object type
                ]
                
                for cache in all_caches:
                    keys_to_remove = []
                    
                    for key in cache.keys():
                        # Check if this cache entry might contain the modified object
                        for pattern in patterns_to_check:
                            if pattern in key:
                                keys_to_remove.append(key)
                                break
                    
                    # Remove identified keys
                    for key in keys_to_remove:
                        del cache[key]
                        self.stats["invalidations"] += 1
                        total_invalidated += 1
            
            logger.debug(f"Cache invalidated {total_invalidated} entries for {object_type} ID {object_id}")
            return total_invalidated
            
        except Exception as e:
            logger.warning(f"Cache invalidation error for {object_type} ID {object_id}: {e}")
            return 0
    
    def clear(self) -> None:
        """Clear entire cache."""
        if self.enabled:
            with self.lock:
                for cache in self.caches.values():
                    cache.clear()
                if self.default_cache:
                    self.default_cache.clear()
                # Reset stats
                self.stats.update({"hits": 0, "misses": 0, "evictions": 0, "invalidations": 0})
            logger.info("Cache cleared")
    
    def get_stats(self) -> Dict[str, Any]:
        """Get cache statistics."""
        if not self.enabled:
            return {"enabled": False}
        
        with self.lock:
            total_requests = self.stats["hits"] + self.stats["misses"]
            hit_ratio = (self.stats["hits"] / total_requests * 100) if total_requests > 0 else 0
            
            # Calculate total cache size across all caches
            total_size = sum(len(cache) for cache in self.caches.values())
            if self.default_cache:
                total_size += len(self.default_cache)
            
            return {
                "enabled": True,
                "size": total_size,
                "max_size": self.config.cache.max_items,
                "hit_ratio_percent": round(hit_ratio, 2),
                **self.stats.copy()  # Return copy to avoid external modifications
            }


class EndpointWrapper:
    """
    Wraps a pynetbox Endpoint to inject caching and safety logic.
    
    This class serves as the "Executor" in the dynamic client architecture,
    implementing comprehensive caching, safety checks, and audit logging
    for all NetBox API operations.
    
    Following Gemini's architectural guidance for enterprise-grade API wrapping.
    """
    
    def __init__(self, endpoint, client: 'NetBoxClient', app_name: str = None):
        """
        Initialize EndpointWrapper with pynetbox endpoint and client reference.
        
        Args:
            endpoint: pynetbox.core.endpoint.Endpoint instance
            client: NetBoxClient instance for access to config, cache, logging
            app_name: NetBox app name (e.g., 'dcim', 'ipam') - will be provided by AppWrapper
        """
        self._endpoint = endpoint
        self._client = client
        
        # Construct object type from app and endpoint name
        # endpoint.name provides the endpoint name (e.g., 'manufacturers')
        # app_name will be provided by AppWrapper (e.g., 'dcim')
        endpoint_name = getattr(endpoint, 'name', 'unknown')
        self._app_name = app_name or 'unknown'
        self._obj_type = f"{self._app_name}.{endpoint_name}"
        
        self.cache = self._client.cache
        
        logger.debug(f"EndpointWrapper initialized for {self._obj_type}")
    
    def _serialize_result(self, result):
        """
        Serialize pynetbox objects for caching using Gemini's recommended strategy.
        
        Uses pynetbox's built-in serialize() method for complete data integrity.
        
        Args:
            result: pynetbox object or list of objects
            
        Returns:
            Serialized dictionary or list of dictionaries
        """
        if isinstance(result, list):
            return [item.serialize() if hasattr(item, 'serialize') else dict(item) for item in result]
        if hasattr(result, 'serialize'):
            return result.serialize()
        return result
    
    def filter(self, *args, no_cache=False, **kwargs) -> list:
        """
        Wrapped filter() method with comprehensive caching and optional cache bypass.
        
        Implements Gemini's caching strategy with cache lookup → API call → cache storage.
        Uses *args, **kwargs for universal pynetbox parameter compatibility.
        
        Args:
            *args: Positional arguments for pynetbox filter()
            no_cache: If True, bypass cache and force fresh API call (for conflict detection)
            **kwargs: Keyword arguments for pynetbox filter()
            
        Returns:
            List of serialized objects from cache or API
        """
        # Generate cache key from filter parameters (excluding no_cache)
        filter_kwargs = {k: v for k, v in kwargs.items() if k != 'no_cache'}
        cache_key = self.cache.generate_cache_key(self._obj_type, **filter_kwargs)
        
        # Check cache first (unless bypassing cache)
        if not no_cache:
            cached_result = self.cache.get(cache_key, self._obj_type)
            if cached_result is not None:
                logger.debug(f"CACHE HIT for {self._obj_type} with key: {cache_key}")
                return cached_result
        else:
            logger.debug(f"CACHE BYPASS requested for {self._obj_type} - forcing fresh API call")
        
        # Cache miss or bypass: fetch from API
        if no_cache:
            logger.debug(f"CACHE BYPASS for {self._obj_type}. Fetching fresh from API with params: {filter_kwargs}")
        else:
            logger.debug(f"CACHE MISS for {self._obj_type}. Fetching from API with params: {filter_kwargs}")
        
        live_result = list(self._endpoint.filter(*args, **filter_kwargs))
        
        # Serialize for caching (Gemini's obj.serialize() strategy)
        serialized_result = self._serialize_result(live_result)
        
        # Store in cache (always store, even for no_cache requests to benefit subsequent calls)
        self.cache.set(cache_key, serialized_result, self._obj_type)
        logger.debug(f"Cached {len(serialized_result)} objects for {self._obj_type}")
        
        return serialized_result
    
    def get(self, *args, **kwargs) -> dict:
        """
        Wrapped get() method with caching for single object retrieval.
        
        Args:
            *args: Positional arguments for pynetbox get()
            **kwargs: Keyword arguments for pynetbox get()
            
        Returns:
            Serialized object dictionary or None if not found
        """
        # Generate cache key for get operation
        cache_key = self.cache.generate_cache_key(f"{self._obj_type}:get", **kwargs)
        
        # Check cache first
        cached_result = self.cache.get(cache_key, self._obj_type)
        if cached_result is not None:
            logger.debug(f"CACHE HIT for {self._obj_type}.get() with key: {cache_key}")
            return cached_result
        
        # Cache miss: fetch from API
        logger.debug(f"CACHE MISS for {self._obj_type}.get(). Fetching from API with params: {kwargs}")
        live_result = self._endpoint.get(*args, **kwargs)
        
        if live_result is None:
            logger.debug(f"No object found for {self._obj_type}.get() with params: {kwargs}")
            return None
        
        # Serialize for caching
        serialized_result = self._serialize_result(live_result)
        
        # Store in cache
        self.cache.set(cache_key, serialized_result, self._obj_type)
        logger.debug(f"Cached single object for {self._obj_type}")
        
        return serialized_result
    
    def all(self, *args, **kwargs) -> list:
        """
        Wrapped all() method with caching for complete object listing.
        
        Args:
            *args: Positional arguments for pynetbox all()
            **kwargs: Keyword arguments for pynetbox all()
            
        Returns:
            List of serialized objects from cache or API
        """
        # Generate cache key for all operation
        cache_key = self.cache.generate_cache_key(f"{self._obj_type}:all", **kwargs)
        
        # Check cache first
        cached_result = self.cache.get(cache_key, self._obj_type)
        if cached_result is not None:
            logger.debug(f"CACHE HIT for {self._obj_type}.all() with key: {cache_key}")
            return cached_result
        
        # Cache miss: fetch from API
        logger.debug(f"CACHE MISS for {self._obj_type}.all(). Fetching from API")
        live_result = list(self._endpoint.all(*args, **kwargs))
        
        # Serialize for caching
        serialized_result = self._serialize_result(live_result)
        
        # Store in cache
        self.cache.set(cache_key, serialized_result, self._obj_type)
        logger.debug(f"Cached {len(serialized_result)} objects for {self._obj_type}.all()")
        
        return serialized_result
    
    def create(self, confirm: bool = False, **payload) -> dict:
        """
        Wrapped create() method with comprehensive safety mechanisms.
        
        Implements Gemini's safety strategy with confirm=True enforcement,
        dry-run integration, and type-based cache invalidation.
        
        Args:
            confirm: Required safety confirmation (must be True)
            **payload: Object data for creation (natural pynetbox-style parameters)
            
        Returns:
            Serialized created object dictionary
            
        Raises:
            NetBoxConfirmationError: If confirm=True not provided
            NetBoxError: For API or validation errors
        """
        # Import exceptions locally to avoid circular imports
        from .exceptions import NetBoxConfirmationError, NetBoxError
        
        # Check 1: Per-call confirmation requirement (Gemini's safety pattern)
        if not confirm:
            raise NetBoxConfirmationError(
                f"create operation on {self._obj_type} requires confirm=True"
            )
        
        # Check 2: Global Dry-Run mode (Gemini's integration pattern)
        if self._client.config.safety.dry_run_mode:
            logger.info(f"[DRY-RUN] Would CREATE {self._obj_type} with payload: {payload}")
            # Return simulated response for dry-run
            return {"id": "dry-run-generated-id", **payload}
        
        try:
            # Execute real operation
            logger.info(f"Creating {self._obj_type} with data: {payload}")
            result = self._endpoint.create(**payload)
            
            # Serialize result for return
            serialized_result = self._serialize_result(result)
            
            # Type-based cache invalidation (Gemini's recommended strategy)
            self._client.cache.invalidate_pattern(self._obj_type)
            logger.info(f"Cache invalidated for {self._obj_type} after create operation")
            
            logger.info(f"✅ Successfully created {self._obj_type} with ID: {result.id}")
            return serialized_result
            
        except Exception as e:
            error_msg = f"Failed to create {self._obj_type}: {e}"
            logger.error(error_msg)
            raise NetBoxError(error_msg)
    
    def update(self, obj_id: int, confirm: bool = False, **payload) -> dict:
        """
        Wrapped update() method with comprehensive safety mechanisms.
        
        Args:
            obj_id: ID of object to update
            confirm: Required safety confirmation (must be True)
            **payload: Object data for update
            
        Returns:
            Serialized updated object dictionary
            
        Raises:
            NetBoxConfirmationError: If confirm=True not provided
            NetBoxError: For API or validation errors
        """
        # Import exceptions locally
        from .exceptions import NetBoxConfirmationError, NetBoxError
        
        # Check 1: Per-call confirmation requirement
        if not confirm:
            raise NetBoxConfirmationError(
                f"update operation on {self._obj_type} requires confirm=True"
            )
        
        # Check 2: Global Dry-Run mode
        if self._client.config.safety.dry_run_mode:
            logger.info(f"[DRY-RUN] Would UPDATE {self._obj_type} ID {obj_id} with payload: {payload}")
            # Return simulated response for dry-run
            return {"id": obj_id, **payload}
        
        try:
            # Get the object to update
            obj_to_update = self._endpoint.get(obj_id)
            if not obj_to_update:
                raise NetBoxError(f"{self._obj_type} with ID {obj_id} not found")
            
            # Execute update operation
            logger.info(f"Updating {self._obj_type} ID {obj_id} with data: {payload}")
            
            # Update the object
            for key, value in payload.items():
                setattr(obj_to_update, key, value)
            obj_to_update.save()
            
            # Serialize result
            serialized_result = self._serialize_result(obj_to_update)
            
            # Type-based cache invalidation
            self._client.cache.invalidate_pattern(self._obj_type)
            logger.info(f"Cache invalidated for {self._obj_type} after update operation")
            
            logger.info(f"✅ Successfully updated {self._obj_type} ID {obj_id}")
            return serialized_result
            
        except Exception as e:
            error_msg = f"Failed to update {self._obj_type} ID {obj_id}: {e}"
            logger.error(error_msg)
            raise NetBoxError(error_msg)
    
    def delete(self, obj_id: int, confirm: bool = False) -> bool:
        """
        Wrapped delete() method with comprehensive safety mechanisms.
        
        Args:
            obj_id: ID of object to delete
            confirm: Required safety confirmation (must be True)
            
        Returns:
            True if deletion successful
            
        Raises:
            NetBoxConfirmationError: If confirm=True not provided
            NetBoxError: For API or validation errors
        """
        # Import exceptions locally
        from .exceptions import NetBoxConfirmationError, NetBoxError
        
        # Check 1: Per-call confirmation requirement
        if not confirm:
            raise NetBoxConfirmationError(
                f"delete operation on {self._obj_type} requires confirm=True"
            )
        
        # Check 2: Global Dry-Run mode
        if self._client.config.safety.dry_run_mode:
            logger.info(f"[DRY-RUN] Would DELETE {self._obj_type} ID {obj_id}")
            return True  # Simulated success for dry-run
        
        try:
            # Get the object to verify it exists
            obj_to_delete = self._endpoint.get(obj_id)
            if not obj_to_delete:
                raise NetBoxError(f"{self._obj_type} with ID {obj_id} not found")
            
            # Execute delete operation
            logger.info(f"Deleting {self._obj_type} ID {obj_id}")
            obj_to_delete.delete()
            
            # Type-based cache invalidation
            self._client.cache.invalidate_pattern(self._obj_type)
            logger.info(f"Cache invalidated for {self._obj_type} after delete operation")
            
            logger.info(f"✅ Successfully deleted {self._obj_type} ID {obj_id}")
            return True
            
        except Exception as e:
            error_msg = f"Failed to delete {self._obj_type} ID {obj_id}: {e}"
            logger.error(error_msg)
            raise NetBoxError(error_msg)


class AppWrapper:
    """
    Wraps a pynetbox App to navigate to wrapped Endpoints.
    
    This class serves as the "Navigator" in the dynamic client architecture,
    routing from NetBox API applications (dcim, ipam, circuits) to their
    specific endpoints, wrapping each endpoint in EndpointWrapper.
    
    Following Gemini's architectural guidance for robust error handling
    and comprehensive debugging visibility.
    """
    
    def __init__(self, app, client: 'NetBoxClient'):
        """
        Initialize AppWrapper with pynetbox app and client reference.
        
        Args:
            app: pynetbox.core.app.App instance (e.g., dcim, ipam)
            client: NetBoxClient instance for access to config, cache, logging
        """
        self._app = app
        self._client = client
        self._app_name = getattr(app, 'name', 'unknown')
        
        logger.debug(f"AppWrapper initialized for app '{self._app_name}'")
    
    def __getattr__(self, name: str):
        """
        Navigate from app to endpoint with comprehensive error handling.
        
        Implements Gemini's robust error handling pattern with try/except
        and detailed debugging logs for troubleshooting __getattr__ chain.
        
        Args:
            name: Endpoint name (e.g., 'devices', 'manufacturers', 'sites')
            
        Returns:
            EndpointWrapper instance for the requested endpoint
            
        Raises:
            AttributeError: If the endpoint doesn't exist on the app
        """
        logger.debug(f"AppWrapper.__getattr__('{name}') on app '{self._app_name}'")
        
        try:
            # Attempt to get the endpoint from the pynetbox app
            endpoint = getattr(self._app, name)
            
            # More strict validation: check if it's actually a pynetbox Endpoint class
            # pynetbox endpoints have specific attributes and behaviors
            if (hasattr(endpoint, 'filter') and hasattr(endpoint, 'get') and hasattr(endpoint, 'all') and
                hasattr(endpoint, 'name') and hasattr(endpoint, 'api') and
                str(type(endpoint)) == "<class 'pynetbox.core.endpoint.Endpoint'>"):
                
                logger.debug(f"Found valid endpoint '{name}' on app '{self._app_name}'")
                logger.debug(f"Returning EndpointWrapper for '{self._app_name}.{name}'")
                
                # Return wrapped endpoint with app name for proper object type construction
                return EndpointWrapper(endpoint, self._client, app_name=self._app_name)
            else:
                logger.debug(f"Object '{name}' on app '{self._app_name}' is not a valid pynetbox Endpoint")
                logger.debug(f"Object type: {type(endpoint)}")
                
        except AttributeError:
            # Log the attempt for debugging
            logger.debug(f"Endpoint '{name}' not found on app '{self._app_name}'")
        
        # If we reach here, the endpoint doesn't exist or isn't valid
        raise AttributeError(
            f"NetBox API application '{self._app_name}' has no endpoint named '{name}'. "
            f"Available endpoints can be discovered through the NetBox API documentation."
        )


class NetBoxClient:
    """
    NetBox API client with safety-first design for read/write operations.
    
    Provides a comprehensive wrapper around pynetbox with:
    - Connection validation and health checking
    - Comprehensive error handling and translation
    - Read-only operations for data exploration
    - Write operations with mandatory safety controls
    - Dry-run mode support for testing
    """
    
    def __init__(self, config: NetBoxConfig):
        """
        Initialize NetBox client with configuration.
        
        Args:
            config: NetBox configuration object
        """
        self.config = config
        self._api = None
        self._connection_status = None
        self._last_health_check = 0
        
        # Add instance tracking for debugging
        self.instance_id = id(self)
        logger.info(f"INITIALIZING new NetBoxClient instance with ID: {self.instance_id}")
        
        # Initialize cache manager following Gemini's strategy
        self.cache = CacheManager(config)
        
        logger.info(f"Initializing NetBox client for {config.url}")
        
        # Log safety configuration
        if config.safety.dry_run_mode:
            logger.warning("NetBox client initialized in DRY-RUN mode - no actual writes will be performed")
        
        # Initialize connection
        self._initialize_connection()
    
    def _initialize_connection(self):
        """Initialize the pynetbox API connection."""
        try:
            self._api = pynetbox.api(
                url=self.config.url,
                token=self.config.token,
                threading=True  # Enable threading for better performance
            )
            
            # Configure session settings
            self._api.http_session.verify = self.config.verify_ssl
            self._api.http_session.timeout = self.config.timeout
            
            # Add custom headers if configured
            if self.config.custom_headers:
                self._api.http_session.headers.update(self.config.custom_headers)
            
            logger.info("NetBox API connection initialized successfully")
            
        except Exception as e:
            error_msg = f"Failed to initialize NetBox API connection: {e}"
            logger.error(error_msg)
            raise NetBoxConnectionError(error_msg, {"url": self.config.url})
    
    @property
    def api(self) -> pynetbox.api:
        """Get the pynetbox API instance."""
        if self._api is None:
            self._initialize_connection()
        return self._api
    
    def health_check(self, force: bool = False) -> ConnectionStatus:
        """
        Perform health check against NetBox API.
        
        Args:
            force: Force health check even if recently performed
            
        Returns:
            ConnectionStatus: Current connection status
        """
        # Check if we need to perform health check (cache for 60 seconds)
        current_time = time.time()
        if not force and (current_time - self._last_health_check) < 60:
            if self._connection_status:
                return self._connection_status
        
        logger.debug("Performing NetBox health check")
        start_time = time.time()
        
        try:
            # Test basic connectivity and get status
            status_data = self.api.status()
            response_time = (time.time() - start_time) * 1000  # Convert to ms
            
            # Extract version information
            netbox_version = status_data.get('netbox-version')
            python_version = status_data.get('python-version') 
            django_version = status_data.get('django-version')
            plugins = status_data.get('plugins', {})
            
            self._connection_status = ConnectionStatus(
                connected=True,
                version=netbox_version,
                python_version=python_version,
                django_version=django_version, 
                plugins=plugins,
                response_time_ms=response_time
            )
            
            self._last_health_check = current_time
            logger.info(f"Health check successful - NetBox {netbox_version} (response: {response_time:.1f}ms)")
            
            return self._connection_status
            
        except requests.exceptions.ConnectionError as e:
            error_msg = f"Connection failed: {e}"
            logger.error(error_msg)
            self._connection_status = ConnectionStatus(connected=False, error=error_msg)
            raise NetBoxConnectionError(error_msg, {"url": self.config.url})
            
        except requests.exceptions.Timeout as e:
            error_msg = f"Request timed out after {self.config.timeout}s: {e}"
            logger.error(error_msg)
            self._connection_status = ConnectionStatus(connected=False, error=error_msg)
            raise NetBoxConnectionError(error_msg, {"timeout": self.config.timeout})
            
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 401:
                error_msg = "Authentication failed - invalid API token"
                logger.error(error_msg)
                self._connection_status = ConnectionStatus(connected=False, error=error_msg)
                raise NetBoxAuthError(error_msg, {"status_code": 401})
            elif e.response.status_code == 403:
                error_msg = "Permission denied - insufficient API token permissions"
                logger.error(error_msg)
                self._connection_status = ConnectionStatus(connected=False, error=error_msg)
                raise NetBoxPermissionError(error_msg, {"status_code": 403})
            else:
                error_msg = f"HTTP error {e.response.status_code}: {e}"
                logger.error(error_msg)
                self._connection_status = ConnectionStatus(connected=False, error=error_msg)
                raise NetBoxError(error_msg, {"status_code": e.response.status_code})
                
        except Exception as e:
            error_msg = f"Unexpected error during health check: {e}"
            logger.error(error_msg)
            self._connection_status = ConnectionStatus(connected=False, error=error_msg)
            raise NetBoxError(error_msg)
    
    def __getattr__(self, name: str):
        """
        Dynamic proxy to NetBox API applications with comprehensive routing.
        
        This method implements Gemini's dynamic client architecture, providing
        100% NetBox API coverage by routing app requests to AppWrapper instances.
        
        Implements the "Entrypoint" role in the three-component architecture:
        NetBoxClient → AppWrapper → EndpointWrapper
        
        Args:
            name: NetBox API application name (e.g., 'dcim', 'ipam', 'circuits')
            
        Returns:
            AppWrapper instance for the requested application
            
        Raises:
            AttributeError: If the application doesn't exist in the NetBox API
        """
        logger.debug(f"NetBoxClient.__getattr__('{name}') -> routing to AppWrapper")
        
        try:
            # Attempt to get the app from the pynetbox API
            app = getattr(self.api, name)
            
            # Validate that it's actually a pynetbox App
            if hasattr(app, 'name') and hasattr(app, 'models') and str(type(app)) == "<class 'pynetbox.core.app.App'>":
                logger.debug(f"Found valid NetBox API app '{name}'")
                logger.debug(f"Returning AppWrapper for '{name}'")
                
                # Return wrapped app for navigation to endpoints
                return AppWrapper(app, self)
            else:
                logger.debug(f"Object '{name}' is not a valid pynetbox App")
                
        except AttributeError:
            logger.debug(f"NetBox API application '{name}' not found")
        
        # If we reach here, the app doesn't exist or isn't valid
        raise AttributeError(
            f"NetBox API has no application named '{name}'. "
            f"Available applications include: dcim, ipam, circuits, extras, tenancy, users, virtualization, wireless"
        )
    

    # WRITE OPERATIONS - SAFETY CRITICAL SECTION
    # =====================================================================
    
    def _check_write_safety(self, operation: str, confirm: bool = False) -> None:
        """
        Verify write operation safety requirements.
        
        Args:
            operation: Name of the write operation
            confirm: Confirmation parameter from caller
            
        Raises:
            NetBoxConfirmationError: If confirm=False
            NetBoxDryRunError: If in dry-run mode (for logging)
        """
        if not confirm:
            error_msg = f"Write operation '{operation}' requires confirm=True for safety"
            logger.error(f"🚨 SAFETY VIOLATION: {error_msg}")
            raise NetBoxConfirmationError(error_msg)
        
        if self.config.safety.dry_run_mode:
            logger.warning(f"🔍 DRY-RUN MODE: Would execute {operation} (no actual changes)")
            # Don't raise error, just log - we'll simulate the operation
    
    def _log_write_operation(self, operation: str, object_type: str, data: Dict[str, Any], 
                           result: Any = None, error: Exception = None) -> None:
        """
        Log write operations for audit trail.
        
        Args:
            operation: Type of operation (create, update, delete)
            object_type: NetBox object type being modified
            data: Data being written or object being modified
            result: Result of the operation (if successful)
            error: Exception if operation failed
        """
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S UTC", time.gmtime())
        
        if error:
            logger.error(f"📝 WRITE FAILED [{timestamp}] {operation.upper()} {object_type}: {error}")
            logger.error(f"📝 Data: {data}")
        else:
            logger.info(f"📝 WRITE SUCCESS [{timestamp}] {operation.upper()} {object_type}")
            logger.info(f"📝 Data: {data}")
            if result and hasattr(result, 'id'):
                logger.info(f"📝 Result ID: {result.id}")
    

    def ensure_manufacturer(
        self,
        name: Optional[str] = None,
        slug: Optional[str] = None,
        description: Optional[str] = None,
        manufacturer_id: Optional[int] = None,
        confirm: bool = False
    ) -> Dict[str, Any]:
        """
        Ensure a manufacturer exists with idempotent behavior using hybrid pattern.
        
        Supports both hierarchical convenience and direct ID injection for performance:
        - Hierarchical: ensure_manufacturer(name="Cisco Systems", confirm=True)
        - Direct ID: ensure_manufacturer(manufacturer_id=5, confirm=True)
        
        Args:
            name: Manufacturer name (required if manufacturer_id not provided)
            slug: URL slug (auto-generated from name if not provided)
            description: Optional description
            manufacturer_id: Direct manufacturer ID (skips lookup if provided)
            confirm: Safety confirmation (REQUIRED: must be True)
            
        Returns:
            Dict containing manufacturer data and operation details
            
        Raises:
            NetBoxValidationError: Invalid input parameters
            NetBoxConfirmationError: Missing confirm=True
            NetBoxNotFoundError: manufacturer_id provided but doesn't exist
            NetBoxWriteError: API operation failed
        """
        operation = "ENSURE_MANUFACTURER"
        
        try:
            # Safety check - ensure confirmation
            self._check_write_safety(operation, confirm)
            
            # Input validation - either name or manufacturer_id must be provided
            if not name and not manufacturer_id:
                raise NetBoxValidationError("Either 'name' or 'manufacturer_id' parameter is required")
            
            if manufacturer_id and name:
                logger.warning(f"Both manufacturer_id ({manufacturer_id}) and name ('{name}') provided. Using manufacturer_id.")
            
            # Pattern B: Direct ID injection (performance path)
            if manufacturer_id:
                try:
                    existing_obj = self.api.dcim.manufacturers.get(manufacturer_id)
                    if not existing_obj:
                        raise NetBoxNotFoundError(f"Manufacturer with ID {manufacturer_id} not found")
                    
                    result_dict = self._object_to_dict(existing_obj)
                    return {
                        "success": True,
                        "action": "unchanged",
                        "object_type": "manufacturer", 
                        "manufacturer": result_dict,
                        "changes": {
                            "created_fields": [],
                            "updated_fields": [],
                            "unchanged_fields": list(result_dict.keys())
                        },
                        "dry_run": False
                    }
                except Exception as e:
                    if "not found" in str(e).lower():
                        raise NetBoxNotFoundError(f"Manufacturer with ID {manufacturer_id} not found")
                    else:
                        raise NetBoxWriteError(f"Failed to retrieve manufacturer {manufacturer_id}: {e}")
            
            # Pattern A: Hierarchical lookup and create (convenience path)
            if not name or not name.strip():
                raise NetBoxValidationError("Manufacturer name cannot be empty")
            
            name = name.strip()
            
            # Check if manufacturer already exists by name
            try:
                existing_manufacturers = list(self.api.dcim.manufacturers.filter(name=name))
                
                if existing_manufacturers:
                    existing_obj = existing_manufacturers[0]
                    existing_dict = self._object_to_dict(existing_obj)
                    
                    # Build desired state for comparison
                    desired_state = {"name": name}
                    if slug:
                        desired_state["slug"] = slug
                    if description:
                        desired_state["description"] = description
                    
                    # Issue #12: Enhanced selective field comparison with hash diffing
                    # First try quick hash comparison
                    if self._hash_comparison_check(existing_dict, desired_state, "manufacturers"):
                        # Hash matches - no update needed, return unchanged
                        logger.debug(f"Hash match for manufacturer '{name}' - no update needed")
                        return {
                            "success": True,
                            "action": "unchanged",
                            "object_type": "manufacturer",
                            "manufacturer": existing_dict,
                            "changes": {
                                "created_fields": [],
                                "updated_fields": [],
                                "unchanged_fields": list(self.MANAGED_FIELDS["manufacturers"])
                            },
                            "dry_run": False
                        }
                    
                    # Hash differs - perform detailed selective field comparison
                    comparison = self._compare_managed_fields(existing_dict, desired_state, "manufacturers")
                    
                    if comparison["needs_update"]:
                        # Prepare update with metadata tracking
                        update_data = self._prepare_metadata_update(desired_state, "manufacturers", "update")
                        
                        logger.info(f"Updating manufacturer '{name}' - managed fields changed: {[f['field'] for f in comparison['updated_fields']]}")
                        result = self.update_object("manufacturers", existing_obj.id, update_data, confirm=True)
                        
                        # Cache invalidation for manufacturer update
                        self.cache.invalidate_pattern("dcim.manufacturer")
                        
                        return {
                            "success": True,
                            "action": "updated",
                            "object_type": "manufacturer",
                            "manufacturer": result,
                            "changes": {
                                "created_fields": [],
                                "updated_fields": [f["field"] for f in comparison["updated_fields"]],
                                "unchanged_fields": comparison["unchanged_fields"]
                            },
                            "dry_run": result.get("dry_run", False)
                        }
                    else:
                        # No changes needed - hash mismatch but field comparison shows no changes
                        logger.info(f"Manufacturer '{name}' already exists with desired state")
                        return {
                            "success": True,
                            "action": "unchanged",
                            "object_type": "manufacturer",
                            "manufacturer": existing_dict,
                            "changes": {
                                "created_fields": [],
                                "updated_fields": [],
                                "unchanged_fields": comparison["unchanged_fields"]
                            },
                            "dry_run": False
                        }
                
                else:
                    # Create new manufacturer with metadata tracking
                    logger.info(f"Creating new manufacturer '{name}'")
                    create_data = {"name": name}
                    if slug:
                        create_data["slug"] = slug
                    if description:
                        create_data["description"] = description
                    
                    # Add metadata for new objects
                    create_data = self._prepare_metadata_update(create_data, "manufacturers", "create")
                    
                    result = self.create_object("manufacturers", create_data, confirm=True)
                    
                    # Cache invalidation for manufacturer creation
                    self.cache.invalidate_pattern("dcim.manufacturer")
                    
                    return {
                        "success": True,
                        "action": "created",
                        "object_type": "manufacturer",
                        "manufacturer": result,
                        "changes": {
                            "created_fields": list(create_data.keys()),
                            "updated_fields": [],
                            "unchanged_fields": []
                        },
                        "dry_run": result.get("dry_run", False)
                    }
                    
            except (NetBoxConfirmationError, NetBoxValidationError, NetBoxNotFoundError):
                raise
            except Exception as e:
                raise NetBoxWriteError(f"Failed to ensure manufacturer '{name}': {e}")
                
        except (NetBoxConfirmationError, NetBoxValidationError, NetBoxNotFoundError, NetBoxWriteError):
            raise
        except Exception as e:
            logger.error(f"Unexpected error in ensure_manufacturer: {e}")
            raise NetBoxError(f"Unexpected error ensuring manufacturer: {e}")


    def ensure_site(
        self,
        name: Optional[str] = None,
        slug: Optional[str] = None,
        status: str = "active",
        region: Optional[str] = None,
        description: Optional[str] = None,
        physical_address: Optional[str] = None,
        site_id: Optional[int] = None,
        confirm: bool = False
    ) -> Dict[str, Any]:
        """
        Ensure a site exists with idempotent behavior using hybrid pattern.
        
        Supports both hierarchical convenience and direct ID injection for performance:
        - Hierarchical: ensure_site(name="Datacenter Amsterdam", confirm=True)
        - Direct ID: ensure_site(site_id=10, confirm=True)
        
        Args:
            name: Site name (required if site_id not provided)
            slug: URL slug (auto-generated from name if not provided)
            status: Site status (default: "active")
            region: Optional region name
            description: Optional description
            physical_address: Optional physical address
            site_id: Direct site ID (skips lookup if provided)
            confirm: Safety confirmation (REQUIRED: must be True)
            
        Returns:
            Dict containing site data and operation details
        """
        operation = "ENSURE_SITE"
        
        try:
            # Safety check - ensure confirmation
            self._check_write_safety(operation, confirm)
            
            # Input validation
            if not name and not site_id:
                raise NetBoxValidationError("Either 'name' or 'site_id' parameter is required")
            
            if site_id and name:
                logger.warning(f"Both site_id ({site_id}) and name ('{name}') provided. Using site_id.")
            
            # Pattern B: Direct ID injection (performance path)
            if site_id:
                try:
                    existing_obj = self.api.dcim.sites.get(site_id)
                    if not existing_obj:
                        raise NetBoxNotFoundError(f"Site with ID {site_id} not found")
                    
                    result_dict = self._object_to_dict(existing_obj)
                    return {
                        "success": True,
                        "action": "unchanged",
                        "object_type": "site",
                        "site": result_dict,
                        "changes": {
                            "created_fields": [],
                            "updated_fields": [],
                            "unchanged_fields": list(result_dict.keys())
                        },
                        "dry_run": False
                    }
                except Exception as e:
                    if "not found" in str(e).lower():
                        raise NetBoxNotFoundError(f"Site with ID {site_id} not found")
                    else:
                        raise NetBoxWriteError(f"Failed to retrieve site {site_id}: {e}")
            
            # Pattern A: Hierarchical lookup and create (convenience path)
            if not name or not name.strip():
                raise NetBoxValidationError("Site name cannot be empty")
            
            name = name.strip()
            
            # Check if site already exists by name
            try:
                existing_sites = list(self.api.dcim.sites.filter(name=name))
                
                if existing_sites:
                    existing_obj = existing_sites[0]
                    existing_dict = self._object_to_dict(existing_obj)
                    
                    # Build desired state for comparison
                    desired_state = {"name": name, "status": status}
                    if slug:
                        desired_state["slug"] = slug
                    if region:
                        desired_state["region"] = region
                    if description:
                        desired_state["description"] = description
                    if physical_address:
                        desired_state["physical_address"] = physical_address
                    
                    # Issue #12: Enhanced selective field comparison with hash diffing
                    # First try quick hash comparison
                    if self._hash_comparison_check(existing_dict, desired_state, "sites"):
                        # Hash matches - no update needed, return unchanged
                        logger.debug(f"Hash match for site '{name}' - no update needed")
                        return {
                            "success": True,
                            "action": "unchanged",
                            "object_type": "site",
                            "site": existing_dict,
                            "changes": {
                                "created_fields": [],
                                "updated_fields": [],
                                "unchanged_fields": list(self.MANAGED_FIELDS["sites"])
                            },
                            "dry_run": False
                        }
                    
                    # Hash differs - perform detailed selective field comparison
                    comparison = self._compare_managed_fields(existing_dict, desired_state, "sites")
                    
                    if comparison["needs_update"]:
                        # Prepare update with metadata tracking
                        update_data = self._prepare_metadata_update(desired_state, "sites", "update")
                        
                        logger.info(f"Updating site '{name}' - managed fields changed: {[f['field'] for f in comparison['updated_fields']]}")
                        result = self.update_object("sites", existing_obj.id, update_data, confirm=True)
                        
                        # Cache invalidation for site update
                        self.cache.invalidate_pattern("dcim.site")
                        
                        return {
                            "success": True,
                            "action": "updated",
                            "object_type": "site",
                            "site": result,
                            "changes": {
                                "created_fields": [],
                                "updated_fields": [f["field"] for f in comparison["updated_fields"]],
                                "unchanged_fields": comparison["unchanged_fields"]
                            },
                            "dry_run": result.get("dry_run", False)
                        }
                    else:
                        # No changes needed - hash mismatch but field comparison shows no changes
                        logger.info(f"Site '{name}' already exists with desired state")
                        return {
                            "success": True,
                            "action": "unchanged",
                            "object_type": "site",
                            "site": existing_dict,
                            "changes": {
                                "created_fields": [],
                                "updated_fields": [],
                                "unchanged_fields": comparison["unchanged_fields"]
                            },
                            "dry_run": False
                        }
                
                else:
                    # Create new site with metadata tracking
                    logger.info(f"Creating new site '{name}'")
                    create_data = {"name": name, "status": status}
                    if slug:
                        create_data["slug"] = slug
                    if region:
                        create_data["region"] = region
                    if description:
                        create_data["description"] = description
                    if physical_address:
                        create_data["physical_address"] = physical_address
                    
                    # Add metadata for new objects
                    create_data = self._prepare_metadata_update(create_data, "sites", "create")
                    
                    result = self.create_object("sites", create_data, confirm=True)
                    
                    # Cache invalidation for site creation
                    self.cache.invalidate_pattern("dcim.site")
                    
                    return {
                        "success": True,
                        "action": "created",
                        "object_type": "site",
                        "site": result,
                        "changes": {
                            "created_fields": list(create_data.keys()),
                            "updated_fields": [],
                            "unchanged_fields": []
                        },
                        "dry_run": result.get("dry_run", False)
                    }
                    
            except (NetBoxConfirmationError, NetBoxValidationError, NetBoxNotFoundError):
                raise
            except Exception as e:
                raise NetBoxWriteError(f"Failed to ensure site '{name}': {e}")
                
        except (NetBoxConfirmationError, NetBoxValidationError, NetBoxNotFoundError, NetBoxWriteError):
            raise
        except Exception as e:
            logger.error(f"Unexpected error in ensure_site: {e}")
            raise NetBoxError(f"Unexpected error ensuring site: {e}")


    def ensure_device_role(
        self,
        name: Optional[str] = None,
        slug: Optional[str] = None,
        color: str = "9e9e9e",
        vm_role: bool = False,
        description: Optional[str] = None,
        role_id: Optional[int] = None,
        confirm: bool = False
    ) -> Dict[str, Any]:
        """
        Ensure a device role exists with idempotent behavior using hybrid pattern.
        
        Supports both hierarchical convenience and direct ID injection for performance:
        - Hierarchical: ensure_device_role(name="Access Switch", confirm=True)
        - Direct ID: ensure_device_role(role_id=3, confirm=True)
        
        Args:
            name: Device role name (required if role_id not provided)
            slug: URL slug (auto-generated from name if not provided)
            color: Hex color code (default: gray)
            vm_role: Whether this role applies to virtual machines
            description: Optional description
            role_id: Direct device role ID (skips lookup if provided)
            confirm: Safety confirmation (REQUIRED: must be True)
            
        Returns:
            Dict containing device role data and operation details
        """
        operation = "ENSURE_DEVICE_ROLE"
        
        try:
            # Safety check - ensure confirmation
            self._check_write_safety(operation, confirm)
            
            # Input validation
            if not name and not role_id:
                raise NetBoxValidationError("Either 'name' or 'role_id' parameter is required")
            
            if role_id and name:
                logger.warning(f"Both role_id ({role_id}) and name ('{name}') provided. Using role_id.")
            
            # Pattern B: Direct ID injection (performance path)
            if role_id:
                try:
                    existing_obj = self.api.dcim.device_roles.get(role_id)
                    if not existing_obj:
                        raise NetBoxNotFoundError(f"Device role with ID {role_id} not found")
                    
                    result_dict = self._object_to_dict(existing_obj)
                    return {
                        "success": True,
                        "action": "unchanged",
                        "object_type": "device_role",
                        "device_role": result_dict,
                        "changes": {
                            "created_fields": [],
                            "updated_fields": [],
                            "unchanged_fields": list(result_dict.keys())
                        },
                        "dry_run": False
                    }
                except Exception as e:
                    if "not found" in str(e).lower():
                        raise NetBoxNotFoundError(f"Device role with ID {role_id} not found")
                    else:
                        raise NetBoxWriteError(f"Failed to retrieve device role {role_id}: {e}")
            
            # Pattern A: Hierarchical lookup and create (convenience path)
            if not name or not name.strip():
                raise NetBoxValidationError("Device role name cannot be empty")
            
            name = name.strip()
            
            # Check if device role already exists by name
            try:
                existing_roles = list(self.api.dcim.device_roles.filter(name=name))
                
                if existing_roles:
                    existing_obj = existing_roles[0]
                    existing_dict = self._object_to_dict(existing_obj)
                    
                    # Build desired state for comparison
                    desired_state = {"name": name, "color": color, "vm_role": vm_role}
                    if slug:
                        desired_state["slug"] = slug
                    if description:
                        desired_state["description"] = description
                    
                    # Issue #12: Enhanced selective field comparison with hash diffing
                    # First try quick hash comparison
                    if self._hash_comparison_check(existing_dict, desired_state, "device_roles"):
                        # Hash matches - no update needed, return unchanged
                        logger.debug(f"Hash match for device role '{name}' - no update needed")
                        return {
                            "success": True,
                            "action": "unchanged",
                            "object_type": "device_role",
                            "device_role": existing_dict,
                            "changes": {
                                "created_fields": [],
                                "updated_fields": [],
                                "unchanged_fields": list(self.MANAGED_FIELDS["device_roles"])
                            },
                            "dry_run": False
                        }
                    
                    # Hash differs - perform detailed selective field comparison
                    comparison = self._compare_managed_fields(existing_dict, desired_state, "device_roles")
                    
                    if comparison["needs_update"]:
                        # Prepare update with metadata tracking
                        update_data = self._prepare_metadata_update(desired_state, "device_roles", "update")
                        
                        logger.info(f"Updating device role '{name}' - managed fields changed: {[f['field'] for f in comparison['updated_fields']]}")
                        result = self.update_object("device_roles", existing_obj.id, update_data, confirm=True)
                        
                        # Cache invalidation for device role update
                        self.cache.invalidate_pattern("dcim.device_role")
                        
                        return {
                            "success": True,
                            "action": "updated",
                            "object_type": "device_role",
                            "device_role": result,
                            "changes": {
                                "created_fields": [],
                                "updated_fields": [f["field"] for f in comparison["updated_fields"]],
                                "unchanged_fields": comparison["unchanged_fields"]
                            },
                            "dry_run": result.get("dry_run", False)
                        }
                    else:
                        # No changes needed - hash mismatch but field comparison shows no changes
                        logger.info(f"Device role '{name}' already exists with desired state")
                        return {
                            "success": True,
                            "action": "unchanged",
                            "object_type": "device_role",
                            "device_role": existing_dict,
                            "changes": {
                                "created_fields": [],
                                "updated_fields": [],
                                "unchanged_fields": comparison["unchanged_fields"]
                            },
                            "dry_run": False
                        }
                
                else:
                    # Create new device role with metadata tracking
                    logger.info(f"Creating new device role '{name}'")
                    create_data = {"name": name, "color": color, "vm_role": vm_role}
                    if slug:
                        create_data["slug"] = slug
                    if description:
                        create_data["description"] = description
                    
                    # Add metadata for new objects
                    create_data = self._prepare_metadata_update(create_data, "device_roles", "create")
                    
                    result = self.create_object("device_roles", create_data, confirm=True)
                    
                    # Cache invalidation for device role creation
                    self.cache.invalidate_pattern("dcim.device_role")
                    
                    return {
                        "success": True,
                        "action": "created",
                        "object_type": "device_role",
                        "device_role": result,
                        "changes": {
                            "created_fields": list(create_data.keys()),
                            "updated_fields": [],
                            "unchanged_fields": []
                        },
                        "dry_run": result.get("dry_run", False)
                    }
                    
            except (NetBoxConfirmationError, NetBoxValidationError, NetBoxNotFoundError):
                raise
            except Exception as e:
                raise NetBoxWriteError(f"Failed to ensure device role '{name}': {e}")
                
        except (NetBoxConfirmationError, NetBoxValidationError, NetBoxNotFoundError, NetBoxWriteError):
            raise
        except Exception as e:
            logger.error(f"Unexpected error in ensure_device_role: {e}")
            raise NetBoxError(f"Unexpected error ensuring device role: {e}")
    
    def ensure_device_type(
        self,
        name: Optional[str] = None,
        manufacturer_id: Optional[int] = None,
        slug: Optional[str] = None,
        model: Optional[str] = None,
        description: Optional[str] = None,
        device_type_id: Optional[int] = None,
        batch_id: Optional[str] = None,
        confirm: bool = False
    ) -> Dict[str, Any]:
        """
        Ensure a device type exists with idempotent behavior using hybrid pattern.
        
        Part of Issue #13 Two-Pass Strategy - Pass 1 object creation.
        Requires manufacturer_id from ensure_manufacturer() result.
        
        Args:
            name: Device type name (required if device_type_id not provided)
            manufacturer_id: Manufacturer ID (required for new device types when using name)
            slug: URL slug (auto-generated from name if not provided)
            model: Model number or name (optional)
            description: Optional description
            device_type_id: Direct device type ID (skips lookup if provided)
            batch_id: Batch ID for rollback capability (two-pass operations)
            confirm: Safety confirmation (REQUIRED: must be True)
            
        Returns:
            Dict containing device type data and operation details
            
        Raises:
            NetBoxValidationError: Invalid input parameters
            NetBoxConfirmationError: Missing confirm=True
            NetBoxNotFoundError: device_type_id provided but doesn't exist
            NetBoxWriteError: API operation failed
        """
        operation = "ENSURE_DEVICE_TYPE"
        
        try:
            # Safety check - ensure confirmation
            self._check_write_safety(operation, confirm)
            
            # Input validation
            if not name and not device_type_id:
                raise NetBoxValidationError("Either 'name' or 'device_type_id' parameter is required")
            
            if device_type_id and name:
                logger.warning(f"Both device_type_id ({device_type_id}) and name ('{name}') provided. Using device_type_id.")
            
            # Pattern B: Direct ID injection (performance path)
            if device_type_id:
                try:
                    existing_obj = self.api.dcim.device_types.get(device_type_id)
                    if not existing_obj:
                        raise NetBoxNotFoundError(f"Device type with ID {device_type_id} not found")
                    
                    result_dict = self._object_to_dict(existing_obj)
                    return {
                        "success": True,
                        "action": "unchanged",
                        "object_type": "device_type",
                        "device_type": result_dict,
                        "changes": {
                            "created_fields": [],
                            "updated_fields": [],
                            "unchanged_fields": list(result_dict.keys())
                        },
                        "dry_run": False
                    }
                except Exception as e:
                    if "not found" in str(e).lower():
                        raise NetBoxNotFoundError(f"Device type with ID {device_type_id} not found")
                    else:
                        raise NetBoxWriteError(f"Failed to retrieve device type {device_type_id}: {e}")
            
            # Pattern A: Hierarchical lookup and create (convenience path)
            if not name or not name.strip():
                raise NetBoxValidationError("Device type name cannot be empty")
            
            name = name.strip()
            
            # Validate manufacturer_id is provided for name-based device type operations
            if not manufacturer_id:
                raise NetBoxValidationError("manufacturer_id is required for device type operations")
            
            # Check if device type already exists by name and manufacturer
            try:
                existing_device_types = list(self.api.dcim.device_types.filter(name=name, manufacturer_id=manufacturer_id))
                
                if existing_device_types:
                    existing_obj = existing_device_types[0]
                    existing_dict = self._object_to_dict(existing_obj)
                    
                    # Build desired state for comparison
                    desired_state = {"name": name, "manufacturer": manufacturer_id}
                    if slug:
                        desired_state["slug"] = slug
                    if model:
                        desired_state["model"] = model
                    if description:
                        desired_state["description"] = description
                    
                    # Issue #13: Enhanced selective field comparison with hash diffing
                    # First try quick hash comparison
                    if self._hash_comparison_check(existing_dict, desired_state, "device_types"):
                        # Hash matches - no update needed, return unchanged
                        logger.debug(f"Hash match for device type '{name}' - no update needed")
                        return {
                            "success": True,
                            "action": "unchanged",
                            "object_type": "device_type",
                            "device_type": existing_dict,
                            "changes": {
                                "created_fields": [],
                                "updated_fields": [],
                                "unchanged_fields": list(self.MANAGED_FIELDS["device_types"])
                            },
                            "dry_run": False
                        }
                    
                    # Hash differs - perform detailed selective field comparison
                    comparison = self._compare_managed_fields(existing_dict, desired_state, "device_types")
                    
                    if comparison["needs_update"]:
                        # Prepare update with metadata tracking
                        update_data = self._prepare_metadata_update(desired_state, "device_types", "update", batch_id)
                        
                        logger.info(f"Updating device type '{name}' - managed fields changed: {[f['field'] for f in comparison['updated_fields']]}")
                        result = self.update_object("device_types", existing_obj.id, update_data, confirm=True)
                        
                        # Cache invalidation for device type update
                        self.cache.invalidate_pattern("dcim.device_type")
                        
                        return {
                            "success": True,
                            "action": "updated",
                            "object_type": "device_type",
                            "device_type": result,
                            "changes": {
                                "created_fields": [],
                                "updated_fields": [f["field"] for f in comparison["updated_fields"]],
                                "unchanged_fields": comparison["unchanged_fields"]
                            },
                            "dry_run": result.get("dry_run", False)
                        }
                    else:
                        # No changes needed - hash mismatch but field comparison shows no changes
                        logger.info(f"Device type '{name}' already exists with desired state")
                        return {
                            "success": True,
                            "action": "unchanged",
                            "object_type": "device_type",
                            "device_type": existing_dict,
                            "changes": {
                                "created_fields": [],
                                "updated_fields": [],
                                "unchanged_fields": comparison["unchanged_fields"]
                            },
                            "dry_run": False
                        }
                
                else:
                    # Create new device type with metadata tracking
                    logger.info(f"Creating new device type '{name}' for manufacturer {manufacturer_id}")
                    create_data = {"name": name, "manufacturer": manufacturer_id}
                    if slug:
                        create_data["slug"] = slug
                    if model:
                        create_data["model"] = model
                    if description:
                        create_data["description"] = description
                    
                    # Add metadata for new objects
                    create_data = self._prepare_metadata_update(create_data, "device_types", "create", batch_id)
                    
                    result = self.create_object("device_types", create_data, confirm=True)
                    
                    # Cache invalidation for device type creation
                    self.cache.invalidate_pattern("dcim.device_type")
                    
                    return {
                        "success": True,
                        "action": "created",
                        "object_type": "device_type",
                        "device_type": result,
                        "changes": {
                            "created_fields": list(create_data.keys()),
                            "updated_fields": [],
                            "unchanged_fields": []
                        },
                        "dry_run": result.get("dry_run", False)
                    }
                    
            except (NetBoxConfirmationError, NetBoxValidationError, NetBoxNotFoundError):
                raise
            except Exception as e:
                logger.error(f"Unexpected error in ensure_device_type: {e}")
                raise NetBoxWriteError(f"Failed to ensure device type: {e}")
                
        except (NetBoxConfirmationError, NetBoxValidationError, NetBoxNotFoundError, NetBoxWriteError):
            raise
        except Exception as e:
            logger.error(f"Unexpected error in ensure_device_type: {e}")
            raise NetBoxError(f"Unexpected error ensuring device type: {e}")
    
    def ensure_device(
        self,
        name: Optional[str] = None,
        device_type_id: Optional[int] = None,
        site_id: Optional[int] = None,
        role_id: Optional[int] = None,
        platform: Optional[str] = None,
        status: str = "active",
        description: Optional[str] = None,
        device_id: Optional[int] = None,
        batch_id: Optional[str] = None,
        confirm: bool = False
    ) -> Dict[str, Any]:
        """
        Ensure a device exists with idempotent behavior using hybrid pattern.
        
        Part of Issue #13 Two-Pass Strategy - Pass 2 relationship object creation.
        Requires device_type_id, site_id, and role_id from Pass 1 results.
        
        Args:
            name: Device name (required if device_id not provided)
            device_type_id: Device type ID (required, from ensure_device_type)
            site_id: Site ID (required, from ensure_site)
            role_id: Device role ID (required, from ensure_device_role)
            platform: Platform/OS name (optional)
            status: Device status (default: active)
            description: Optional description
            device_id: Direct device ID (skips lookup if provided)
            batch_id: Batch ID for rollback capability (two-pass operations)
            confirm: Safety confirmation (REQUIRED: must be True)
            
        Returns:
            Dict containing device data and operation details
            
        Raises:
            NetBoxValidationError: Invalid input parameters
            NetBoxConfirmationError: Missing confirm=True
            NetBoxNotFoundError: device_id provided but doesn't exist
            NetBoxWriteError: API operation failed
        """
        operation = "ENSURE_DEVICE"
        
        try:
            # Safety check - ensure confirmation
            self._check_write_safety(operation, confirm)
            
            # Input validation
            if not name and not device_id:
                raise NetBoxValidationError("Either 'name' or 'device_id' parameter is required")
            
            if device_id and name:
                logger.warning(f"Both device_id ({device_id}) and name ('{name}') provided. Using device_id.")
            
            # Pattern B: Direct ID injection (performance path)
            if device_id:
                try:
                    existing_obj = self.api.dcim.devices.get(device_id)
                    if not existing_obj:
                        raise NetBoxNotFoundError(f"Device with ID {device_id} not found")
                    
                    result_dict = self._object_to_dict(existing_obj)
                    return {
                        "success": True,
                        "action": "unchanged",
                        "object_type": "device",
                        "device": result_dict,
                        "changes": {
                            "created_fields": [],
                            "updated_fields": [],
                            "unchanged_fields": list(result_dict.keys())
                        },
                        "dry_run": False
                    }
                except Exception as e:
                    if "not found" in str(e).lower():
                        raise NetBoxNotFoundError(f"Device with ID {device_id} not found")
                    else:
                        raise NetBoxWriteError(f"Failed to retrieve device {device_id}: {e}")
            
            # Pattern A: Hierarchical lookup and create (convenience path)
            if not name or not name.strip():
                raise NetBoxValidationError("Device name cannot be empty")
            
            # Validate required dependencies for device creation
            if not device_type_id:
                raise NetBoxValidationError("device_type_id is required for device operations")
            if not site_id:
                raise NetBoxValidationError("site_id is required for device operations")
            if not role_id:
                raise NetBoxValidationError("role_id is required for device operations")
            
            name = name.strip()
            
            # Check if device already exists by name and site
            try:
                existing_devices = list(self.api.dcim.devices.filter(name=name, site_id=site_id))
                
                if existing_devices:
                    existing_obj = existing_devices[0]
                    existing_dict = self._object_to_dict(existing_obj)
                    
                    # Build desired state for comparison
                    desired_state = {
                        "name": name,
                        "device_type": device_type_id,
                        "site": site_id,
                        "role": role_id,
                        "status": status
                    }
                    if platform:
                        desired_state["platform"] = platform
                    if description:
                        desired_state["description"] = description
                    
                    # Issue #12: Enhanced selective field comparison with hash diffing
                    comparison_result = self._compare_managed_fields(
                        existing_dict, desired_state, "devices"
                    )
                    
                    # Generate metadata
                    metadata = self._generate_metadata(batch_id, "devices")
                    
                    if comparison_result["needs_update"]:
                        # Update required
                        logger.info(f"Device '{name}' exists but requires updates: {comparison_result['changed_fields']}")
                        
                        # Merge desired state with metadata
                        update_data = {**desired_state, **metadata}
                        
                        result = self.update_object(
                            object_type="devices",
                            object_id=existing_obj.id,
                            data=update_data,
                            confirm=confirm
                        )
                        
                        # Cache invalidation for device update
                        self.cache.invalidate_pattern("dcim.device") 
                        
                        return {
                            "success": True,
                            "action": "updated",
                            "object_type": "device",
                            "device": result,
                            "changes": {
                                "created_fields": [],
                                "updated_fields": comparison_result["changed_fields"],
                                "unchanged_fields": comparison_result["unchanged_fields"]
                            },
                            "dry_run": False
                        }
                    
                    else:
                        # No changes needed
                        logger.info(f"Device '{name}' already exists with desired state")
                        return {
                            "success": True,
                            "action": "unchanged",
                            "object_type": "device",
                            "device": existing_dict,
                            "changes": {
                                "created_fields": [],
                                "updated_fields": [],
                                "unchanged_fields": comparison_result["unchanged_fields"]
                            },
                            "dry_run": False
                        }
                
                # Device doesn't exist, create it
                logger.info(f"Creating new device '{name}' in site {site_id}")
                
                # Prepare creation data
                create_data = {
                    "name": name,
                    "device_type": device_type_id,
                    "site": site_id,
                    "role": role_id,
                    "status": status
                }
                
                # Add optional fields
                if platform:
                    create_data["platform"] = platform
                if description:
                    create_data["description"] = description
                
                # Add metadata
                metadata = self._generate_metadata(batch_id, "devices")
                create_data.update(metadata)
                
                result = self.create_object(
                    object_type="devices",
                    data=create_data,
                    confirm=confirm
                )
                
                # Cache invalidation for device creation
                self.cache.invalidate_pattern("dcim.device")
                
                return {
                    "success": True,
                    "action": "created",
                    "object_type": "device",
                    "device": result,
                    "changes": {
                        "created_fields": list(create_data.keys()),
                        "updated_fields": [],
                        "unchanged_fields": []
                    },
                    "dry_run": False
                }
                
            except (NetBoxConfirmationError, NetBoxValidationError, NetBoxNotFoundError, NetBoxWriteError):
                raise
            except Exception as e:
                logger.error(f"API error during device lookup: {e}")
                raise NetBoxWriteError(f"Failed to query devices: {e}")
        
        except (NetBoxConfirmationError, NetBoxValidationError, NetBoxNotFoundError, NetBoxWriteError):
            raise
        except Exception as e:
            logger.error(f"Unexpected error in ensure_device: {e}")
            raise NetBoxError(f"Unexpected error ensuring device: {e}")


class NetBoxBulkOrchestrator:
    """
    Stateless orchestrator for two-pass NetBox bulk operations.
    
    Architecture based on Gemini's guidance:
    - Absolutely stateless per operation - no persistent state between operations
    - Strict DAG dependency structure: Manufacturer → DeviceType → Device  
    - Object cache contains full pynetbox objects (not just IDs) for optimization
    - batch_id tracking for robust rollback capability
    - Pre-flight report generation with detailed diff analysis
    """
    
    # Strict dependency graph - defines processing order for Pass 1
    DEPENDENCY_ORDER = [
        'manufacturers',    # No dependencies
        'sites',           # No dependencies  
        'device_roles',    # No dependencies
        'device_types',    # Depends on manufacturers
        'devices'          # Depends on device_types, sites, device_roles
    ]
    
    def __init__(self, netbox_client: 'NetBoxClient'):
        """
        Initialize stateless orchestrator for single bulk operation.
        
        Args:
            netbox_client: NetBox client instance for API operations
        """
        self.client = netbox_client
        
        # Object cache: {object_type: {name: full_pynetbox_object}}
        # Contains full objects for optimization (avoid extra API calls)
        self.object_cache = {
            'manufacturers': {},
            'sites': {},
            'device_roles': {},
            'device_types': {},
            'devices': {},
            'interfaces': {},
            'ip_addresses': {}
        }
        
        # Operation tracking
        self.batch_id = self._generate_batch_id()
        self.normalized_data = {}
        self.pre_flight_report = {}
        
        # Results tracking
        self.results = {
            "pass_1": {"created": [], "updated": [], "unchanged": [], "errors": []},
            "pass_2": {"created": [], "updated": [], "unchanged": [], "errors": []},
            "summary": {}
        }
        
        logger.info(f"NetBoxBulkOrchestrator initialized (stateless) with batch_id: {self.batch_id}")
    
    def _generate_batch_id(self) -> str:
        """Generate unique batch ID for rollback tracking."""
        import uuid
        from datetime import datetime
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        batch_uuid = str(uuid.uuid4())[:8]
        batch_id = f"batch_{timestamp}_{batch_uuid}"
        
        logger.info(f"Generated batch ID: {batch_id}")
        return batch_id
    
    def normalize_bulk_data(self, devices_data: List[Dict[str, Any]]) -> Dict[str, List]:
        """
        Parse & Normalize: Convert nested JSON to flat lists for DAG processing.
        
        Following Gemini's guidance: normalize complex nested structures into 
        separate lists that can be processed in dependency order.
        
        Args:
            devices_data: List of raw device data with nested relationships
            
        Returns:
            Normalized flat lists by object type for DAG processing
        """
        logger.info(f"Normalizing {len(devices_data)} devices for two-pass processing")
        
        normalized = {
            'manufacturers': [],
            'sites': [],
            'device_roles': [],
            'device_types': [],
            'devices': [],
            'interfaces': [],
            'ip_addresses': []
        }
        
        # Track seen objects to avoid duplicates
        seen = {obj_type: set() for obj_type in normalized.keys()}
        
        for device_data in devices_data:
            # Extract manufacturers
            if device_data.get("manufacturer"):
                manufacturer_name = device_data["manufacturer"]
                if manufacturer_name not in seen['manufacturers']:
                    normalized['manufacturers'].append({
                        "name": manufacturer_name,
                        "slug": manufacturer_name.lower().replace(" ", "-"),
                        "batch_id": self.batch_id
                    })
                    seen['manufacturers'].add(manufacturer_name)
            
            # Extract sites
            if device_data.get("site"):
                site_name = device_data["site"]
                if site_name not in seen['sites']:
                    normalized['sites'].append({
                        "name": site_name,
                        "slug": site_name.lower().replace(" ", "-"),
                        "status": "active",
                        "batch_id": self.batch_id
                    })
                    seen['sites'].add(site_name)
            
            # Extract device roles
            if device_data.get("role"):
                role_name = device_data["role"]
                if role_name not in seen['device_roles']:
                    normalized['device_roles'].append({
                        "name": role_name,
                        "slug": role_name.lower().replace(" ", "-"),
                        "color": "9e9e9e",  # Default gray
                        "vm_role": False,
                        "batch_id": self.batch_id
                    })
                    seen['device_roles'].add(role_name)
            
            # Extract device types
            if device_data.get("device_type") and device_data.get("manufacturer"):
                device_type_key = f"{device_data['manufacturer']}::{device_data['device_type']}"
                if device_type_key not in seen['device_types']:
                    normalized['device_types'].append({
                        "name": device_data["device_type"],
                        "manufacturer": device_data["manufacturer"],
                        "model": device_data.get("model", device_data["device_type"]),
                        "slug": device_data["device_type"].lower().replace(" ", "-"),
                        "description": device_data.get("device_type_description", ""),
                        "batch_id": self.batch_id
                    })
                    seen['device_types'].add(device_type_key)
            
            # Add devices (these have dependencies)
            normalized['devices'].append({
                "name": device_data["name"],
                "device_type": device_data.get("device_type"),
                "manufacturer": device_data.get("manufacturer"), 
                "site": device_data.get("site"),
                "role": device_data.get("role"),
                "platform": device_data.get("platform"),
                "status": device_data.get("status", "active"),
                "description": device_data.get("description", ""),
                "batch_id": self.batch_id
            })
            
            # Extract interfaces (Pass 2 objects)
            for interface_data in device_data.get("interfaces", []):
                normalized['interfaces'].append({
                    **interface_data,
                    "device_name": device_data["name"],
                    "batch_id": self.batch_id
                })
            
            # Extract IP addresses (Pass 2 objects)
            for ip_data in device_data.get("ip_addresses", []):
                normalized['ip_addresses'].append({
                    **ip_data,
                    "device_name": device_data["name"],
                    "batch_id": self.batch_id
                })
        
        # Log normalization results
        for obj_type, objects in normalized.items():
            logger.info(f"Normalized {len(objects)} {obj_type}")
            
        self.normalized_data = normalized
        return normalized
    
    def generate_pre_flight_report(self, dry_run: bool = True) -> Dict[str, Any]:
        """
        Generate detailed pre-flight report of all operations that would be performed.
        
        Critical safety mechanism per Gemini's guidance: Always generate a diff 
        before any real operations to enable analysis and confirmation.
        
        Returns:
            Detailed report of CREATE/UPDATE/DELETE operations planned
        """
        logger.info("Generating pre-flight report for bulk operation")
        
        if not self.normalized_data:
            raise NetBoxValidationError("No normalized data available. Call normalize_bulk_data() first.")
        
        report = {
            "batch_id": self.batch_id,
            "summary": {"CREATE": 0, "UPDATE": 0, "UNCHANGED": 0, "TOTAL": 0},
            "operations": [],
            "warnings": [],
            "validation_errors": []
        }
        
        # Simulate operations for each object type in dependency order
        for obj_type in self.DEPENDENCY_ORDER:
            if obj_type in self.normalized_data and self.normalized_data[obj_type]:
                type_operations = self._analyze_object_type(obj_type, self.normalized_data[obj_type])
                report["operations"].extend(type_operations)
                
                # Update summary counts
                for op in type_operations:
                    action = op.get("planned_action", "UNKNOWN")
                    if action in report["summary"]:
                        report["summary"][action] += 1
                    report["summary"]["TOTAL"] += 1
        
        self.pre_flight_report = report
        logger.info(f"Pre-flight report generated: {report['summary']}")
        return report
    
    def _analyze_object_type(self, obj_type: str, objects: List[Dict]) -> List[Dict]:
        """Analyze what operations would be performed for objects of a specific type."""
        operations = []
        
        for obj_data in objects:
            try:
                # Check if object exists
                existing_obj = self._find_existing_object(obj_type, obj_data)
                
                if existing_obj:
                    # Object exists - analyze if update needed
                    needs_update, changes = self._analyze_changes(obj_type, existing_obj, obj_data)
                    operations.append({
                        "object_type": obj_type,
                        "name": obj_data.get("name", "unknown"),
                        "planned_action": "UPDATE" if needs_update else "UNCHANGED",
                        "existing_id": existing_obj.id,
                        "changes": changes if needs_update else {},
                        "batch_id": obj_data.get("batch_id")
                    })
                    
                    # Cache the existing object with full pynetbox object
                    self.object_cache[obj_type][obj_data["name"]] = existing_obj
                    
                else:
                    # Object doesn't exist - will be created
                    operations.append({
                        "object_type": obj_type,
                        "name": obj_data.get("name", "unknown"),
                        "planned_action": "CREATE",
                        "new_data": obj_data,
                        "batch_id": obj_data.get("batch_id")
                    })
                    
            except Exception as e:
                operations.append({
                    "object_type": obj_type,
                    "name": obj_data.get("name", "unknown"),
                    "planned_action": "ERROR",
                    "error": str(e)
                })
        
        return operations
    
    def _find_existing_object(self, obj_type: str, obj_data: Dict) -> Optional[Any]:
        """Find existing NetBox object by name/key, return full pynetbox object."""
        name = obj_data.get("name")
        if not name:
            return None
        
        try:
            if obj_type == "manufacturers":
                results = self.client._api.dcim.manufacturers.filter(name=name)
            elif obj_type == "sites":
                results = self.client._api.dcim.sites.filter(name=name)
            elif obj_type == "device_roles":
                results = self.client._api.dcim.device_roles.filter(name=name)
            elif obj_type == "device_types":
                # Device types are unique by name + manufacturer
                manufacturer_name = obj_data.get("manufacturer")
                if manufacturer_name:
                    manufacturer = self.client._api.dcim.manufacturers.filter(name=manufacturer_name)
                    if manufacturer:
                        results = self.client._api.dcim.device_types.filter(
                            model=name, 
                            manufacturer_id=manufacturer[0].id
                        )
                    else:
                        return None
                else:
                    return None
            elif obj_type == "devices":
                results = self.client._api.dcim.devices.filter(name=name)
            else:
                return None
            
            return results[0] if results else None
            
        except Exception as e:
            logger.warning(f"Error finding existing {obj_type} '{name}': {e}")
            return None
    
    def _analyze_changes(self, obj_type: str, existing_obj: Any, new_data: Dict) -> tuple[bool, Dict]:
        """Analyze what changes would be made to existing object."""
        changes = {}
        
        # Compare managed fields only (following selective field comparison pattern)
        managed_fields = self.client.MANAGED_FIELDS.get(obj_type, {})
        
        for field_name, field_config in managed_fields.items():
            new_value = new_data.get(field_name)
            existing_value = getattr(existing_obj, field_name, None)
            
            # Handle different field types
            if field_config.get("type") == "reference":
                # For reference fields, resolve to ID for comparison
                if new_value and existing_value:
                    if hasattr(existing_value, 'id'):
                        existing_value = existing_value.id
                    # TODO: Resolve new_value to ID based on reference type
            
            if new_value is not None and new_value != existing_value:
                changes[field_name] = {
                    "from": existing_value,
                    "to": new_value
                }
        
        return len(changes) > 0, changes
    
    def execute_pass_1(self, confirm: bool = False) -> Dict[str, Any]:
        """
        Execute Pass 1: Process core objects in strict DAG dependency order.
        
        Following Gemini's guidance: Process manufacturers → sites → device_roles → 
        device_types → devices in that exact order to avoid dependency issues.
        
        Args:
            confirm: Whether to execute changes (safety mechanism)
            
        Returns:
            Pass 1 results with processing statistics
        """
        logger.info("Starting Pass 1: DAG-ordered core objects processing")
        
        if not self.normalized_data:
            raise NetBoxValidationError("No normalized data available. Call normalize_bulk_data() first.")
        
        # Process each object type in strict dependency order
        for obj_type in self.DEPENDENCY_ORDER:
            if obj_type in self.normalized_data and self.normalized_data[obj_type]:
                objects = self.normalized_data[obj_type]
                logger.info(f"Processing {len(objects)} {obj_type}")
                
                for obj_data in objects:
                    try:
                        result = self._process_object(obj_type, obj_data, confirm)
                        self._record_result("pass_1", result)
                        
                        # Cache full pynetbox object for optimization
                        obj_name = obj_data["name"]
                        if result.get("action") in ["created", "updated", "unchanged"]:
                            obj_key = f"{obj_type}:{obj_name}"
                            netbox_obj = result.get(obj_type.rstrip('s'))  # Remove 's' from plural
                            if netbox_obj:
                                self.object_cache[obj_type][obj_name] = netbox_obj
                        
                    except Exception as e:
                        error_result = {
                            "object_type": obj_type,
                            "name": obj_data.get("name", "unknown"),
                            "error": str(e)
                        }
                        self.results["pass_1"]["errors"].append(error_result)
                        logger.error(f"Pass 1 {obj_type} error: {e}")
                        
                        # Continue processing other objects rather than failing entirely
                        continue
        
        # Generate summary
        total_processed = sum(len(self.results["pass_1"][action]) for action in ["created", "updated", "unchanged"])
        total_errors = len(self.results["pass_1"]["errors"])
        
        logger.info(f"Pass 1 completed: {total_processed} objects processed, {total_errors} errors")
        
        return {
            "objects_processed": total_processed,
            "errors": total_errors,
            "cache_size": sum(len(cache) for cache in self.object_cache.values()),
            "results": self.results["pass_1"]
        }
    
    def _process_object(self, obj_type: str, obj_data: Dict, confirm: bool) -> Dict[str, Any]:
        """Process individual object using appropriate ensure method."""
        obj_name = obj_data["name"]
        
        # Use cached object if available (from pre-flight analysis)
        if obj_name in self.object_cache[obj_type]:
            existing_obj = self.object_cache[obj_type][obj_name]
            
            # Check if update needed using selective field comparison
            needs_update, changes = self._analyze_changes(obj_type, existing_obj, obj_data)
            
            if not needs_update:
                return {
                    "action": "unchanged",
                    obj_type.rstrip('s'): existing_obj,
                    "message": f"{obj_type.rstrip('s').title()} '{obj_name}' is up to date"
                }
        
        # Process based on object type using existing ensure methods
        if obj_type == "manufacturers":
            return self.client.ensure_manufacturer(
                name=obj_name,
                slug=obj_data.get("slug"),
                description=obj_data.get("description", ""),
                batch_id=obj_data.get("batch_id"),
                confirm=confirm
            )
            
        elif obj_type == "sites":
            return self.client.ensure_site(
                name=obj_name,
                slug=obj_data.get("slug"),
                status=obj_data.get("status", "active"),
                description=obj_data.get("description", ""),
                batch_id=obj_data.get("batch_id"),
                confirm=confirm
            )
            
        elif obj_type == "device_roles":
            return self.client.ensure_device_role(
                name=obj_name,
                slug=obj_data.get("slug"),
                color=obj_data.get("color", "9e9e9e"),
                vm_role=obj_data.get("vm_role", False),
                description=obj_data.get("description", ""),
                batch_id=obj_data.get("batch_id"),
                confirm=confirm
            )
            
        elif obj_type == "device_types":
            # Device types need manufacturer_id resolved
            manufacturer_name = obj_data.get("manufacturer")
            manufacturer_obj = self.object_cache["manufacturers"].get(manufacturer_name)
            
            if not manufacturer_obj:
                raise NetBoxValidationError(f"Device type '{obj_name}' requires manufacturer '{manufacturer_name}' to be processed first")
            
            return self.client.ensure_device_type(
                name=obj_name,
                manufacturer_id=manufacturer_obj.id,
                model=obj_data.get("model"),
                slug=obj_data.get("slug"),
                description=obj_data.get("description", ""),
                batch_id=obj_data.get("batch_id"),
                confirm=confirm
            )
            
        elif obj_type == "devices":
            # Devices need multiple dependencies resolved
            device_type_name = obj_data.get("device_type")
            site_name = obj_data.get("site")
            role_name = obj_data.get("role")
            
            device_type_obj = self.object_cache["device_types"].get(device_type_name)
            site_obj = self.object_cache["sites"].get(site_name)
            role_obj = self.object_cache["device_roles"].get(role_name)
            
            missing_deps = []
            if not device_type_obj and device_type_name:
                missing_deps.append(f"device_type '{device_type_name}'")
            if not site_obj and site_name:
                missing_deps.append(f"site '{site_name}'")
            if not role_obj and role_name:
                missing_deps.append(f"device_role '{role_name}'")
                
            if missing_deps:
                raise NetBoxValidationError(f"Device '{obj_name}' missing dependencies: {', '.join(missing_deps)}")
            
            return self.client.ensure_device(
                name=obj_name,
                device_type_id=device_type_obj.id if device_type_obj else None,
                site_id=site_obj.id if site_obj else None,
                role_id=role_obj.id if role_obj else None,
                platform=obj_data.get("platform"),
                status=obj_data.get("status", "active"),
                description=obj_data.get("description", ""),
                batch_id=obj_data.get("batch_id"),
                confirm=confirm
            )
            
        else:
            raise NetBoxValidationError(f"Unknown object type: {obj_type}")
    
    def execute_pass_2(self, normalized_data: Dict[str, Any], pass_1_results: Dict[str, Any], confirm: bool = False) -> Dict[str, Any]:
        """
        Execute Pass 2: Create relationship objects using Pass 1 IDs.
        
        Args:
            normalized_data: Normalized device data
            pass_1_results: Results from Pass 1 with object IDs
            confirm: Whether to execute changes (safety mechanism)
            
        Returns:
            Pass 2 results
        """
        logger.info("Starting Pass 2: Relationship objects creation")
        relationship_objects = normalized_data["relationship_objects"]
        pass_2_results = {}
        
        # 1. Ensure Device (primary relationship object)
        device_data = relationship_objects.get("device", {})
        if device_data and device_data.get("name"):
            try:
                # Use Pass 1 results for dependencies
                device_type_id = pass_1_results.get("device_type_id") or self._resolve_device_type_id(device_data.get("device_type"))
                site_id = pass_1_results.get("site_id") or self._resolve_site_id(device_data.get("site"))
                role_id = pass_1_results.get("device_role_id") or self._resolve_device_role_id(device_data.get("role"))
                
                if not all([device_type_id, site_id, role_id]):
                    missing = []
                    if not device_type_id: missing.append("device_type_id")
                    if not site_id: missing.append("site_id") 
                    if not role_id: missing.append("role_id")
                    raise NetBoxValidationError(f"Device creation requires: {', '.join(missing)}")
                
                device_result = self.client.ensure_device(
                    name=device_data["name"],
                    device_type_id=device_type_id,
                    site_id=site_id,
                    role_id=role_id,
                    platform=device_data.get("platform"),
                    status=device_data.get("status", "active"),
                    description=device_data.get("description"),
                    batch_id=self.batch_id,
                    confirm=confirm
                )
                pass_2_results["device_id"] = device_result["device"]["id"]
                self._record_result("pass_2", device_result)
                self.operation_cache[f"device:{device_data['name']}"] = device_result["device"]["id"]
                
            except Exception as e:
                error_result = {"object_type": "device", "name": device_data.get("name"), "error": str(e)}
                self.results["pass_2"]["errors"].append(error_result)
                logger.error(f"Pass 2 device error: {e}")
                raise NetBoxError(f"Pass 2 failed creating device: {e}")
        
        # Note: Interfaces and IP addresses would be implemented here
        # Skipping for now as we focus on device-level two-pass strategy
        
        logger.info(f"Pass 2 completed successfully. Created {len(pass_2_results)} relationship objects")
        return pass_2_results
    
    def _record_result(self, pass_name: str, operation_result: Dict[str, Any]):
        """Record operation result in appropriate pass category."""
        action = operation_result.get("action", "unknown")
        if action in ["created", "updated", "unchanged"]:
            self.results[pass_name][action].append(operation_result)
        else:
            logger.warning(f"Unknown action '{action}' in operation result")
    
    def _resolve_manufacturer_id(self, manufacturer_name: str) -> Optional[int]:
        """Resolve manufacturer name to ID using cache or API lookup."""
        if not manufacturer_name:
            return None
            
        cache_key = f"manufacturer:{manufacturer_name}"
        if cache_key in self.operation_cache:
            return self.operation_cache[cache_key]
        
        # Fallback to API lookup
        try:
            manufacturers = self.client._api.dcim.manufacturers.filter(name=manufacturer_name)
            if manufacturers:
                manufacturer_id = manufacturers[0].id
                self.operation_cache[cache_key] = manufacturer_id
                return manufacturer_id
        except Exception as e:
            logger.warning(f"Failed to resolve manufacturer '{manufacturer_name}': {e}")
        
        return None
    
    def _resolve_site_id(self, site_name: str) -> Optional[int]:
        """Resolve site name to ID using cache or API lookup."""
        if not site_name:
            return None
            
        cache_key = f"site:{site_name}"
        if cache_key in self.operation_cache:
            return self.operation_cache[cache_key]
        
        # Fallback to API lookup
        try:
            sites = self.client._api.dcim.sites.filter(name=site_name)
            if sites:
                site_id = sites[0].id
                self.operation_cache[cache_key] = site_id
                return site_id
        except Exception as e:
            logger.warning(f"Failed to resolve site '{site_name}': {e}")
        
        return None
    
    def _resolve_device_role_id(self, role_name: str) -> Optional[int]:
        """Resolve device role name to ID using cache or API lookup."""
        if not role_name:
            return None
            
        cache_key = f"device_role:{role_name}"
        if cache_key in self.operation_cache:
            return self.operation_cache[cache_key]
        
        # Fallback to API lookup
        try:
            roles = self.client._api.dcim.device_roles.filter(name=role_name)
            if roles:
                role_id = roles[0].id
                self.operation_cache[cache_key] = role_id
                return role_id
        except Exception as e:
            logger.warning(f"Failed to resolve device role '{role_name}': {e}")
        
        return None
    
    def _resolve_device_type_id(self, device_type_name: str) -> Optional[int]:
        """Resolve device type name to ID using cache or API lookup."""
        if not device_type_name:
            return None
            
        cache_key = f"device_type:{device_type_name}"
        if cache_key in self.operation_cache:
            return self.operation_cache[cache_key]
        
        # Fallback to API lookup
        try:
            device_types = self.client._api.dcim.device_types.filter(name=device_type_name)
            if device_types:
                device_type_id = device_types[0].id
                self.operation_cache[cache_key] = device_type_id
                return device_type_id
        except Exception as e:
            logger.warning(f"Failed to resolve device type '{device_type_name}': {e}")
        
        return None
    
    def generate_operation_report(self) -> Dict[str, Any]:
        """
        Generate comprehensive report of two-pass operation results.
        
        Returns:
            Detailed report with statistics and change summary
        """
        total_pass_1 = sum(len(self.results["pass_1"][action]) for action in ["created", "updated", "unchanged"])
        total_pass_2 = sum(len(self.results["pass_2"][action]) for action in ["created", "updated", "unchanged"])
        total_errors = len(self.results["pass_1"]["errors"]) + len(self.results["pass_2"]["errors"])
        
        report = {
            "batch_id": self.batch_id,
            "operation_summary": {
                "total_objects_processed": total_pass_1 + total_pass_2,
                "total_errors": total_errors,
                "success_rate": round((total_pass_1 + total_pass_2) / (total_pass_1 + total_pass_2 + total_errors) * 100, 2) if (total_pass_1 + total_pass_2 + total_errors) > 0 else 100
            },
            "pass_1_summary": {
                "core_objects_processed": total_pass_1,
                "created": len(self.results["pass_1"]["created"]),
                "updated": len(self.results["pass_1"]["updated"]),
                "unchanged": len(self.results["pass_1"]["unchanged"]),
                "errors": len(self.results["pass_1"]["errors"])
            },
            "pass_2_summary": {
                "relationship_objects_processed": total_pass_2,
                "created": len(self.results["pass_2"]["created"]),
                "updated": len(self.results["pass_2"]["updated"]),
                "unchanged": len(self.results["pass_2"]["unchanged"]),
                "errors": len(self.results["pass_2"]["errors"])
            },
            "detailed_results": self.results,
            "cache_statistics": {
                "cached_objects": len(self.operation_cache),
                "cache_keys": list(self.operation_cache.keys())
            }
        }
        
        return report